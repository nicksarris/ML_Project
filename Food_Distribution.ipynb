{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data = pd.read_csv(\"Capital_Area_Food_Bank_Hunger_Estimates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1039, 42)\n",
      "Index(['OBJECTID', 'STATEFP10', 'COUNTYFP10', 'TRACTCE10', 'GEOID10', 'NAME10',\n",
      "       'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10',\n",
      "       'GEOGRAPH', 'TRACT', 'POVERTY200', 'TRACTCE', 'PERCENT_CA',\n",
      "       'PERCENT_TR', 'PERCENT_WA', 'TOTAL_POP', 'UNEMPLOYME', 'POVERTY_RA',\n",
      "       'MEDIAN_INC', 'PERCENT_BL', 'PERCENT_HI', 'HOME_OWN', 'F15_FI_RATE',\n",
      "       'F15_FI_POP', 'F15_LB_NEED', 'F15_LB_UNME', 'F15_DISTRIB', 'F15_PPIN',\n",
      "       'FY_FI_RATE', 'FY_FI_POP', 'FY_LB_UNME', 'FY_DISTRIB', 'FY_PPIN',\n",
      "       'F14_FI_RATE', 'F14_LB_UNME', 'F14_DISTRIB', 'F14_PPIN', 'SHAPEAREA',\n",
      "       'SHAPELEN'],\n",
      "      dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [OBJECTID, STATEFP10, COUNTYFP10, TRACTCE10, GEOID10, NAME10, NAMELSAD10, MTFCC10, FUNCSTAT10, ALAND10, AWATER10, GEOGRAPH, TRACT, POVERTY200, TRACTCE, PERCENT_CA, PERCENT_TR, PERCENT_WA, TOTAL_POP, UNEMPLOYME, POVERTY_RA, MEDIAN_INC, PERCENT_BL, PERCENT_HI, HOME_OWN, F15_FI_RATE, F15_FI_POP, F15_LB_NEED, F15_LB_UNME, F15_DISTRIB, F15_PPIN, FY_FI_RATE, FY_FI_POP, FY_LB_UNME, FY_DISTRIB, FY_PPIN, F14_FI_RATE, F14_LB_UNME, F14_DISTRIB, F14_PPIN, SHAPEAREA, SHAPELEN]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "print(food_data.shape)\n",
    "print(food_data.columns)\n",
    "print(food_data[food_data.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1039 rows and 42 columns.\n",
      "   OBJECTID  STATEFP10  COUNTYFP10  TRACTCE10      GEOID10   NAME10  \\\n",
      "0        26         24          31     703402  24031703402  7034.02   \n",
      "1        27         24          31     701202  24031701202  7012.02   \n",
      "\n",
      "             NAMELSAD10 MTFCC10 FUNCSTAT10  ALAND10  ...  FY_FI_POP  \\\n",
      "0  Census Tract 7034.02   G5020          S  1257675  ...   0.000000   \n",
      "1  Census Tract 7012.02   G5020          S  1841629  ...  87.030944   \n",
      "\n",
      "     FY_LB_UNME   FY_DISTRIB    FY_PPIN  F14_FI_RATE   F14_LB_UNME  \\\n",
      "0      0.000000     0.000000   0.000000          2.0  12250.272452   \n",
      "1  14863.288579  3413.209661  39.218346          3.6  17749.107675   \n",
      "\n",
      "   F14_DISTRIB   F14_PPIN     SHAPEAREA     SHAPELEN  \n",
      "0      2038.13  29.954845  1.257565e+06  4579.987688  \n",
      "1      2496.57  25.895904  1.848218e+06  6774.565707  \n",
      "\n",
      "[2 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "print('We have {} rows and {} columns.'.format(food_data.shape[0], food_data.shape[1]))\n",
    "print(food_data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using contains the data from the DC area food banks on the food given out at different food banks over several years. There are features such as what was needed by the community and what was sent to the food bank that year. Overall, it consists of 1032 rows and 42 columns throughout. We'll go in depth about each of these individual columns below and analyze whether or not they're going to be useful to our project throughout.\n",
    "\n",
    "List of Columns: OBJECTID, STATEFP10, COUNTYFP10, TRACTCE10, GEOID10, NAME10, NAMELSAD10, MTFCC10, FUNCSTAT10, ALAND10, AWATER10, GEOGRAPH, TRACT, POVERTY200, TRACTCE, PERCENT_CA, PERCENT_TR, PERCENT_WA, TOTAL_POP, UNEMPLOYME, POVERTY_RA, MEDIAN_INC, PERCENT_BL, PERCENT_HI, HOME_OWN, F15_FI_RATE, F15_FI_POP, F15_LB_NEED, F15_LB_UNME, F15_DISTRIB, F15_PPIN, FY_FI_RATE, FY_FI_POP, FY_LB_UNME, FY_DISTRIB, FY_PPIN, F14_FI_RATE, F14_LB_UNME, F14_DISTRIB, F14_PPIN, SHAPEAREA, SHAPELEN\n",
    "\n",
    "Sadly, looking at the list of columns in the data, we were never really given a description of what some of these features refer to. I'll separate them in the list below based on what information we are able to go and derive based on the name and, following, information that we're unsure about.\n",
    "\n",
    "<ul>\n",
    "    <b>ID Columns</b>\n",
    "    <li>ObjectID (int64): The unique ID of the row/item being discussed</li>\n",
    "    <li>StateFP10 (int64): The state ID of the row/item being discussed</li>\n",
    "    <li>CountyFP10 (int64): The county ID of the row/item being discussed</li>\n",
    "    <li>TractFP10 (int64): The tract ID of the row/item being discussed</li>\n",
    "    <li>GeoID10 (int64): The geographic ID of the row/item being discussed</li>\n",
    "    <li>Name10 (float): The name of the row/item being discussed</li>\n",
    "    <li>NameLSad10 (object): The longer/more descriptive name of the row/item</li>\n",
    "</ul>\n",
    "\n",
    "These columns specifically go and refer to the individual name/ID of the land/tract/etc. being discussed in the row throughout. Each of these values mentioned above act as an identifier that specifically helps to distinguish the item described from those whom might be similar regarding other traits.\n",
    "\n",
    "<ul>\n",
    "    <b>Attribute Columns</b>\n",
    "    <li>ALand10 (int64): Land area of the row/item being discussed</li>\n",
    "    <li>AWater10 (int64): Water area of the row/item being discussed</li>\n",
    "    <li>TotalPop (int64): Total population of the row/item being discussed</li>\n",
    "    <li>Unemployme (float): Unemployment rate of the row/item being discussed</li>\n",
    "    <li>PovertyRa (float): Poverty rate of the row/item being discussed</li>\n",
    "    <li>MedianInc (float): Median income of the row/item being discussed</li>\n",
    "    <li>PercentBl (float): Percentage of African-Americans who live in the row/item being discussed</li>\n",
    "    <li>PercentHi (float): Percentage of Hispanics who live in the row/item being discussed</li>\n",
    "    <li>HomeOwn (float): Percentage of homeowners who live in the row/item being discussed</li>\n",
    "</ul>\n",
    "\n",
    "These columns delve further into the traits/attributes that help to characterize the land/tract/etc. being discussed in the row throughout. These features in particular will probably be some of those that help the most with feature engineering in particular, as they're the ones that specifically differentiate the pieces of land from each other with regards to demographics, helping us to get further insights into the population and individuals that live there.\n",
    "\n",
    "<ul>\n",
    "    <b>Food/Distribution Columns</b>\n",
    "    <li>Fx_FI_Rate (float): The estimated portion of the population in the census tract experiencing food insecurity</li>\n",
    "    <li>Fx_FI_Pop (float): The estimated number of people in the census tract experiencing food insecurity</li>\n",
    "    <li>Fx_LB_Need (float): The estimated pounds of food needed by the food insecure population in the census tract</li>\n",
    "    <li>Fx_Distrib (float): The number of pounds of food distributed by CAFB and partners in the census tract</li>\n",
    "    <li>Fx_LB_Unme (float): The difference between the estimated pounds of food needed and the real pounds of food distributed</li>\n",
    "</ul>\n",
    "\n",
    "These columns are perhaps the most important, as they specifically have to do with the target variable that we'll select/look into. The column 'Fx_LB_Unme' will perhaps be later selected to be the target, as it in particular deals with the either the amount of unnecessary food delivered or the amount of food lacking throughout.\n",
    "\n",
    "<ul>\n",
    "    <b>Unknown Columns</b>\n",
    "    <li>MTFCC10 (object): Seems like another ID value but not completely sure.</li>\n",
    "    <li>FuncStat10 (object): Seems like another either ID value or attribute but not completey sure</li>\n",
    "</ul>\n",
    "\n",
    "Both of these columns don't seem to be that important at all, and don't offer much information throughout from what I can tell. Unless I find out that they're incredibly valuable as a whole when I look into analysis further, I'm probably going to drop them from consideration regarding any models built.\n",
    "\n",
    "<ul>\n",
    "    <b>Unknown Columns</b>\n",
    "    <li>Percent_CA (object): Seems like an attribute column, could be useful for feature engineering</li>\n",
    "    <li>Percent_TR (object): Seems like an attribute column, could be useful for feature engineering</li>\n",
    "    <li>Percent_WA (object): Seems like an attribute column, could be useful for feature engineering</li>\n",
    "</ul>\n",
    "\n",
    "These columns are definitely attributes of a given census tract, but there's no indication of what they stand for. Nevertheless, they'll probably come in handy when taking feature engineering into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NAMELSAD10', 'FUNCSTAT10', 'MTFCC10']"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = food_data._get_numeric_data().columns\n",
    "categorical_columns = list(set(food_data.columns) - set(num_cols))\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, I'm just going to remove each of the columns above. We have no idea what 'MTFCC10' and 'FUNCSTAT10' refer to, and it seems like all the information to be gained from 'NAMELSAD10' is contained in the abbreviated 'NAME' column. There's nothing further to be gained from simply encoding each individual value in 'NAMELSAD10', given that there won't be any overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data.drop(['FUNCSTAT10', 'NAMELSAD10', 'MTFCC10'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides this, we're going to go and drop all those columns that are either referring to ID or just those that we don't specifically know what they mean in regards to their definition. These won't be too helpful in regards to our models throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"F15_LB_UNME\", \"FY_FI_RATE\", \"FY_FI_POP\", \n",
    "                   \"FY_LB_UNME\", \"FY_DISTRIB\", \"FY_PPIN\", \"OBJECTID\", \n",
    "                   \"STATEFP10\", \"COUNTYFP10\", \"TRACTCE10\", \"GEOID10\", \n",
    "                   \"NAME10\"]\n",
    "\n",
    "for col in columns_to_drop:\n",
    "    food_data.drop(col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll go and create a simple model so that we can get a baseline score for the dataset throughout, without any feature engineering or advanced models throughout. I don't expect this score to be any good, given that the established columns provided don't seem too predictive as a whole just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current R2: 0.9982414591315324\n",
      "Current RMSE: 3043.799396200841\n"
     ]
    }
   ],
   "source": [
    "train_y = food_data['F15_LB_NEED'].values\n",
    "train_x = food_data.drop('F15_LB_NEED', axis=1)\n",
    "xtra, xte, ytra, yte = train_test_split(\n",
    "    train_x, train_y, test_size=0.2)\n",
    "\n",
    "rf_params = {}\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(xtra, ytra)\n",
    "train_preds = rf_model.predict(xte)\n",
    "\n",
    "print(\"Current R2: {}\".format(r2_score(yte, train_preds)))\n",
    "print(\"Current RMSE: {}\".format(math.sqrt(mean_squared_error(yte, train_preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like I missed some leakage. There's definitely one column that's extremely predictive that is used to go and calculate the correct amount of food for each census tract. Let's go and look into the RandomForest feature importances and see which feature it happens to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F15_FI_POP</th>\n",
       "      <td>0.991564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            importance\n",
       "F15_FI_POP    0.991564"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(\n",
    "    rf_model.feature_importances_, index = train_x.columns,\n",
    "    columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, one column happens to be extremely predictive; in this case it happens to be \"F15_FI_Pop\", which refers to the estimated number of people in the census tract whom happen to be experiencing food insecurity. I expect what happens is that the individuals whom decide how much food is needed simply multiply the total population in need by a scalar value. Realizing this, we now have to go and change up what we're trying to predict/work with, as there's no point in predicting a static value that explicitly is based off another value. However, although we may still change things up, we'll also go and repeat the same methodology as above, except with the \"F15_FI_Pop\" column removed. This will allow for us to get a feel for the overall predictiveness of the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current R2: 0.9795375418220428\n",
      "Current RMSE: 9530.829425220334\n"
     ]
    }
   ],
   "source": [
    "food_data = food_data.drop([\"F15_FI_POP\", \"F14_LB_UNME\"], axis=1)\n",
    "\n",
    "train_y = food_data['F15_LB_NEED'].values\n",
    "train_x = food_data.drop('F15_LB_NEED', axis=1)\n",
    "xtra, xte, ytra, yte = train_test_split(\n",
    "    train_x, train_y, test_size=0.2)\n",
    "\n",
    "rf_params = {}\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(xtra, ytra)\n",
    "train_preds = rf_model.predict(xte)\n",
    "\n",
    "print(\"Current R2: {}\".format(r2_score(yte, train_preds)))\n",
    "print(\"Current RMSE: {}\".format(math.sqrt(mean_squared_error(yte, train_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F15_FI_RATE</th>\n",
       "      <td>0.451985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F15_DISTRIB</th>\n",
       "      <td>0.211830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL_POP</th>\n",
       "      <td>0.122675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F14_FI_RATE</th>\n",
       "      <td>0.105980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F14_DISTRIB</th>\n",
       "      <td>0.054189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             importance\n",
       "F15_FI_RATE    0.451985\n",
       "F15_DISTRIB    0.211830\n",
       "TOTAL_POP      0.122675\n",
       "F14_FI_RATE    0.105980\n",
       "F14_DISTRIB    0.054189"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(\n",
    "    rf_model.feature_importances_, index = train_x.columns,\n",
    "    columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little more useful, as the overall distribution/rate seems to correlate with the overall need as a whole. We're probably still going to go and modify our target in order to predict a more useful statistic--probably the amount of wasted/unmet food so that CAFB and partners can optimize their overall distribution. Overall, though, we're going to have to go and transform the data, splitting by year so that we can get a sense of how predictive our model truly is. Our new target column will be \"LB_UNME\". We've also decided that we're going to drop all of the attribute/other columns, given their lack of helpfulness to the overall model throughout when looking at the RandomForest feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data = pd.read_csv(\"Capital_Area_Food_Bank_Hunger_Estimates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "f15_columns = ['F15_FI_RATE', 'F15_LB_UNME', 'F15_DISTRIB', 'F15_PPIN']\n",
    "f14_columns = ['F14_FI_RATE', 'F14_LB_UNME', 'F14_DISTRIB', 'F14_PPIN']\n",
    "\n",
    "f14_initial = [col for col in food_data.columns if col not in f14_columns]\n",
    "f14_final = [col for col in f14_initial if col not in f15_columns] + f14_columns\n",
    "f15_initial = [col for col in food_data.columns if col not in f15_columns]\n",
    "f15_final = [col for col in f15_initial if col not in f14_columns] + f15_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data_2014 = food_data[f14_final]\n",
    "food_data_2015 = food_data[f15_final]\n",
    "\n",
    "f14_rename = {'F14_FI_RATE': \"FI_RATE\", 'F14_LB_UNME': \"LB_UNME\", 'F14_DISTRIB': \"DISTRIB\", 'F14_PPIN': \"PPIN\"}\n",
    "f15_rename = {'F15_FI_RATE': \"FI_RATE\", 'F15_LB_UNME': \"LB_UNME\", 'F15_DISTRIB': \"DISTRIB\", 'F15_PPIN': \"PPIN\"}\n",
    "cols_to_use = [\"LB_UNME\", \"FI_RATE\", \"TOTAL_POP\", \"PPIN\", \"DISTRIB\"]\n",
    "\n",
    "food_data_2014 = food_data_2014.rename(columns=f14_rename)[cols_to_use]\n",
    "food_data_2015 = food_data_2015.rename(columns=f15_rename)[cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current R2: 0.9307301324557461\n",
      "Current RMSE: 11015.170945393511\n",
      "Current R2 (2015): -1.4735078589514057\n",
      "Current RMSE (2015): 69461.93916524405\n"
     ]
    }
   ],
   "source": [
    "train_y = food_data_2014['LB_UNME'].values\n",
    "train_x = food_data_2014.drop('LB_UNME', axis=1)\n",
    "xtra, xte, ytra, yte = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=2019)\n",
    "\n",
    "rf_params = {}\n",
    "rf_params[\"n_estimators\"] = 100\n",
    "rf_params[\"max_depth\"] = 5\n",
    "rf_params[\"random_state\"] = 2019\n",
    "\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(xtra, ytra)\n",
    "train_preds = rf_model.predict(xte)\n",
    "\n",
    "test_y = food_data_2015[\"LB_UNME\"]\n",
    "test_x = food_data_2015.drop('LB_UNME', axis=1)\n",
    "test_preds = rf_model.predict(test_x)\n",
    "\n",
    "print(\"Current R2: {}\".format(r2_score(yte, train_preds)))\n",
    "print(\"Current RMSE: {}\".format(math.sqrt(mean_squared_error(yte, train_preds))))\n",
    "print(\"Current R2 (2015): {}\".format(r2_score(test_y, test_preds)))\n",
    "print(\"Current RMSE (2015): {}\".format(math.sqrt(mean_squared_error(test_y, test_preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight off the bat, we can see that our model is pretty awful at generalizing, given the low R2 score and the high RMSE in comparison. The next order of business is to go and work on improving this value so that we can explicitly determine where resources would be best served. In the following cell, we're going to focus on scaling throughout to see if it ends up being helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_unmet = food_data_2014[\"LB_UNME\"].values\n",
    "food_data_2014_scaled = food_data_2014.drop(\"LB_UNME\", axis=1)\n",
    "food_data_2014_scaled = pd.DataFrame(scaler.fit_transform(food_data_2014_scaled))\n",
    "food_data_2014_scaled[\"LB_UNME\"] = train_unmet\n",
    "\n",
    "test_unmet = food_data_2015[\"LB_UNME\"].values\n",
    "food_data_2015_scaled = food_data_2015.drop(\"LB_UNME\", axis=1)\n",
    "food_data_2015_scaled = pd.DataFrame(scaler.fit_transform(food_data_2015_scaled))\n",
    "food_data_2015_scaled[\"LB_UNME\"] = test_unmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current R2: 0.930685333553116\n",
      "Current RMSE: 11018.732290527003\n",
      "Current R2 (2015): 0.8770873438216746\n",
      "Current RMSE (2015): 15484.192364371012\n"
     ]
    }
   ],
   "source": [
    "train_y = food_data_2014_scaled['LB_UNME'].values\n",
    "train_x = food_data_2014_scaled.drop('LB_UNME', axis=1)\n",
    "xtra, xte, ytra, yte = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=2019)\n",
    "\n",
    "rf_params = {}\n",
    "rf_params[\"n_estimators\"] = 100\n",
    "rf_params[\"max_depth\"] = 5\n",
    "rf_params[\"random_state\"] = 2019\n",
    "\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(xtra, ytra)\n",
    "train_preds = rf_model.predict(xte)\n",
    "\n",
    "test_y = food_data_2015_scaled[\"LB_UNME\"]\n",
    "test_x = food_data_2015_scaled.drop('LB_UNME', axis=1)\n",
    "test_preds = rf_model.predict(test_x)\n",
    "\n",
    "print(\"Current R2: {}\".format(r2_score(yte, train_preds)))\n",
    "print(\"Current RMSE: {}\".format(math.sqrt(mean_squared_error(yte, train_preds))))\n",
    "print(\"Current R2 (2015): {}\".format(r2_score(test_y, test_preds)))\n",
    "print(\"Current RMSE (2015): {}\".format(math.sqrt(mean_squared_error(test_y, test_preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are much better. Here we can see that, when scaled, we can actually get a pretty good sense of the target variable throughout, and that our model actually generalizes pretty well when predicting this unknown data. Let's see if we can improve the score further through a bit of feature engineering and/or polynomial features added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 2)\n",
    "\n",
    "train_unmet = food_data_2014_scaled[\"LB_UNME\"].values\n",
    "food_data_2014_poly = food_data_2014_scaled.drop(\"LB_UNME\", axis=1)\n",
    "food_data_2014_poly = pd.DataFrame(poly.fit_transform(food_data_2014_poly))\n",
    "food_data_2014_poly[\"LB_UNME\"] = train_unmet\n",
    "\n",
    "test_unmet = food_data_2015_scaled[\"LB_UNME\"].values\n",
    "food_data_2015_poly = food_data_2015_scaled.drop(\"LB_UNME\", axis=1)\n",
    "food_data_2015_poly = pd.DataFrame(poly.fit_transform(food_data_2015_poly))\n",
    "food_data_2015_poly[\"LB_UNME\"] = train_unmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current R2: 0.9290810974470055\n",
      "Current RMSE: 11145.513083202444\n",
      "Current R2 (2015): 0.7788032604521672\n",
      "Current RMSE (2015): 20245.909858644583\n"
     ]
    }
   ],
   "source": [
    "train_y = food_data_2014_poly['LB_UNME'].values\n",
    "train_x = food_data_2014_poly.drop('LB_UNME', axis=1)\n",
    "xtra, xte, ytra, yte = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=2019)\n",
    "\n",
    "rf_params = {}\n",
    "rf_params[\"n_estimators\"] = 100\n",
    "rf_params[\"max_depth\"] = 5\n",
    "rf_params[\"random_state\"] = 2019\n",
    "\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(xtra, ytra)\n",
    "train_preds = rf_model.predict(xte)\n",
    "\n",
    "test_y = food_data_2015_poly[\"LB_UNME\"]\n",
    "test_x = food_data_2015_poly.drop('LB_UNME', axis=1)\n",
    "test_preds = rf_model.predict(test_x)\n",
    "\n",
    "print(\"Current R2: {}\".format(r2_score(yte, train_preds)))\n",
    "print(\"Current RMSE: {}\".format(math.sqrt(mean_squared_error(yte, train_preds))))\n",
    "print(\"Current R2 (2015): {}\".format(r2_score(test_y, test_preds)))\n",
    "print(\"Current RMSE (2015): {}\".format(math.sqrt(mean_squared_error(test_y, test_preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like these features will end up being helpful throughout, and that there aren't that many useful interactions to be found from polynomial features. Overall, though, given that we've dropped the majority of attributes and the above statement is true, perhaps we should look further into model building and the like to improve our results as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(object):\n",
    "\n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, train, target, splits, mute=False):\n",
    "        \n",
    "        train_preds = np.zeros((len(train)))\n",
    "        \n",
    "        for index, (train_idx, valid_idx) in enumerate(splits):\n",
    "            train_x = np.array(train)[train_idx.astype(int)]\n",
    "            train_y = np.array(target)[train_idx.astype(int)]\n",
    "            valid_x = np.array(train)[valid_idx.astype(int)]\n",
    "            valid_y = np.array(target)[valid_idx.astype(int)]\n",
    "            \n",
    "            self.clf.fit(train_x, train_y)\n",
    "            split_preds = self.clf.predict(valid_x)\n",
    "            train_preds[valid_idx] = split_preds\n",
    "\n",
    "        if mute == False:\n",
    "            print(\"Global Train R2: {}\".format(r2_score(target, train_preds)))\n",
    "            print(\"Global Train RMSE: {}\".format(math.sqrt(mean_squared_error(target, train_preds))))\n",
    "            \n",
    "        return train_preds\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        test_preds = self.clf.predict(x)\n",
    "        return test_preds\n",
    "\n",
    "    def predict_test(self, x, target, mute=False):\n",
    "        \n",
    "        test_preds = self.clf.predict(x)\n",
    "        \n",
    "        if mute == False:\n",
    "            print(\"Global Test R2: {}\".format(r2_score(target, test_preds)))\n",
    "            print(\"Global Test RMSE: {}\".format(math.sqrt(mean_squared_error(target, test_preds))))\n",
    "            \n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "seed = 2019\n",
    "\n",
    "train_y = food_data_2014_scaled['LB_UNME'].values\n",
    "train_x = food_data_2014_scaled.drop('LB_UNME', axis=1)\n",
    "test_y = food_data_2015_scaled[\"LB_UNME\"]\n",
    "test_x = food_data_2015_scaled.drop('LB_UNME', axis=1)\n",
    "\n",
    "splits = list(KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_x, train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the accuracy of our CV, each individual model will be trained and optimized on five different splits of the training data, which again consists of the 2014 data gathered and explored above. The models will specifically predict the amount of unmet need experienced after the 2015 distribution, also as gathered and explored above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.9637160343403612\n",
      "Global Train RMSE: 8199.83874777421\n",
      "Global Test R2: 0.940738521117752\n",
      "Global Test RMSE: 10751.684172598094\n"
     ]
    }
   ],
   "source": [
    "rf_params = {}\n",
    "rf_params['n_estimators'] = 100\n",
    "rf_params['random_state'] = 2019\n",
    "\n",
    "rf_model = SklearnWrapper(clf=RandomForestRegressor, params=rf_params)\n",
    "rf_train_preds = rf_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "rf_test_preds = rf_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.9805915091511428\n",
      "Global Train RMSE: 5997.134953779228\n",
      "Global Test R2: 0.9562164313818313\n",
      "Global Test RMSE: 9241.571632967785\n"
     ]
    }
   ],
   "source": [
    "et_params = {}\n",
    "et_params['n_estimators'] = 100\n",
    "et_params['random_state'] = 2019\n",
    "\n",
    "et_model = SklearnWrapper(clf=ExtraTreesRegressor, params=et_params)\n",
    "et_train_preds = et_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "et_test_preds = et_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.792642078358342\n",
      "Global Train RMSE: 19602.35517671075\n",
      "Global Test R2: 0.7953165206701296\n",
      "Global Test RMSE: 19981.663979803947\n"
     ]
    }
   ],
   "source": [
    "lr_params = {}\n",
    "\n",
    "lr_model = SklearnWrapper(clf=LinearRegression, params=lr_params)\n",
    "lr_train_preds = lr_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "lr_test_preds = lr_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.7926437667265711\n",
      "Global Train RMSE: 19602.27537252289\n",
      "Global Test R2: 0.7952903999113028\n",
      "Global Test RMSE: 19982.93892289358\n"
     ]
    }
   ],
   "source": [
    "ri_params = {}\n",
    "\n",
    "ri_model = SklearnWrapper(clf=Ridge, params=ri_params)\n",
    "ri_train_preds = ri_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "ri_test_preds = ri_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.7926415377189162\n",
      "Global Train RMSE: 19602.3807310736\n",
      "Global Test R2: 0.7953163881350945\n",
      "Global Test RMSE: 19981.670448987774\n"
     ]
    }
   ],
   "source": [
    "la_params = {}\n",
    "\n",
    "la_model = SklearnWrapper(clf=Lasso, params=la_params)\n",
    "la_train_preds = la_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "la_test_preds = la_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.792642078358342\n",
      "Global Train RMSE: 19602.35517671075\n",
      "Global Test R2: 0.7953165206701296\n",
      "Global Test RMSE: 19981.663979803947\n"
     ]
    }
   ],
   "source": [
    "en_params = {}\n",
    "\n",
    "en_model = SklearnWrapper(clf=LinearRegression, params=en_params)\n",
    "en_train_preds = en_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "en_test_preds = en_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.9711114903988048\n",
      "Global Train RMSE: 7316.620113500541\n",
      "Global Test R2: 0.963777594270698\n",
      "Global Test RMSE: 8405.797074801845\n"
     ]
    }
   ],
   "source": [
    "gb_params = {}\n",
    "gb_params['random_state'] = 2019\n",
    "\n",
    "gb_model = SklearnWrapper(clf=GradientBoostingRegressor, params=gb_params)\n",
    "gb_train_preds = gb_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "gb_test_preds = gb_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.8474795246360962\n",
      "Global Train RMSE: 16811.7147822436\n",
      "Global Test R2: 0.8033925258229204\n",
      "Global Test RMSE: 19583.497998894356\n"
     ]
    }
   ],
   "source": [
    "ab_params = {}\n",
    "ab_params['n_estimators'] = 100\n",
    "ab_params['random_state'] = 2019\n",
    "\n",
    "ab_model = SklearnWrapper(clf=AdaBoostRegressor, params=ab_params)\n",
    "ab_train_preds = ab_model.train(train_x, train_y, splits).reshape(-1, 1)\n",
    "ab_test_preds = ab_model.predict_test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Train R2: 0.9814907746492356\n",
      "Global Train RMSE: 5856.552756664617\n",
      "Global Test R2: 0.9682178427428052\n",
      "Global Test RMSE: 7873.755815742421\n"
     ]
    }
   ],
   "source": [
    "train_second_layer = np.concatenate((\n",
    "    train_x,\n",
    "    rf_train_preds, \n",
    "    et_train_preds, \n",
    "    #lr_train_preds, \n",
    "    #ri_train_preds, \n",
    "    #la_train_preds, \n",
    "    #en_train_preds, \n",
    "    gb_train_preds, \n",
    "    #ab_train_preds,\n",
    "), axis=1)\n",
    "\n",
    "test_second_layer = np.concatenate((\n",
    "    test_x,\n",
    "    rf_test_preds[:, None], \n",
    "    et_test_preds[:, None], \n",
    "    #lr_test_preds[:, None], \n",
    "    #ri_test_preds[:, None], \n",
    "    #la_test_preds[:, None], \n",
    "    #en_test_preds[:, None], \n",
    "    gb_test_preds[:, None], \n",
    "    #ab_test_preds[:, None],\n",
    "), axis=1)\n",
    "\n",
    "et2_params = {}\n",
    "et2_params['n_estimators'] = 200\n",
    "et2_params['max_depth'] = 10\n",
    "et2_params['random_state'] = 2019\n",
    "\n",
    "et2_model = SklearnWrapper(clf=ExtraTreesRegressor, params=et2_params)\n",
    "et2_train_preds = et2_model.train(train_second_layer, train_y, splits).reshape(-1, 1)\n",
    "et2_test_preds = et2_model.predict_test(test_second_layer, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finalizing and tuning the stacking ensemble, we were able to reach an R2 score of approximately 0.96821 and an RMSE score of approximately 7873.75 on the test set, quite a bit higher than any of the individual first-layer models throughout. Following this, the next step is to use this model and the information gathered to go and see if we're successfully able to minimize unmet need in the 2015 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59288756.15181693\n"
     ]
    }
   ],
   "source": [
    "sum_unmet = sum(test_y)\n",
    "print(sum_unmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63151744.30407871\n"
     ]
    }
   ],
   "source": [
    "sum_unmet_pred = sum(et2_test_preds)\n",
    "print(sum_unmet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above stacking ensemble made use of three individual model predictions on top of the initial train/test data. The next step is to implement a pipeline to quickly generate scores/predictions, allowing us to modify the distribution as necessary in order to minimize the unmet need throughout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsemblePipeline(object):\n",
    "        \n",
    "    def transform(self, train, test):\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        train_unmet = train[\"LB_UNME\"].values\n",
    "        test_unmet = test[\"LB_UNME\"].values\n",
    "\n",
    "        train = train.drop(\"LB_UNME\", axis=1)\n",
    "        train = pd.DataFrame(scaler.fit_transform(train))\n",
    "        train[\"LB_UNME\"] = train_unmet\n",
    "        \n",
    "        test = test.drop(\"LB_UNME\", axis=1)\n",
    "        test = pd.DataFrame(scaler.fit_transform(test))\n",
    "        test[\"LB_UNME\"] = test_unmet\n",
    "        \n",
    "        return train, test\n",
    "        \n",
    "    def gen_predictions(self, train, test, second_layer_params):\n",
    "        \n",
    "        n_splits = 5\n",
    "        seed = 2019\n",
    "\n",
    "        train_y = train['LB_UNME'].values\n",
    "        train_x = train.drop('LB_UNME', axis=1)\n",
    "        test_y = test[\"LB_UNME\"]\n",
    "        test_x = test.drop('LB_UNME', axis=1)\n",
    "\n",
    "        splits = list(KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_x, train_y))\n",
    "        \n",
    "        rf_model = SklearnWrapper(clf=RandomForestRegressor, params={\n",
    "            'n_estimators': 100, 'random_state': 2019})\n",
    "        rf_train_preds = rf_model.train(train_x, train_y, splits, mute=True).reshape(-1, 1)\n",
    "        rf_test_preds = rf_model.predict_test(test_x, test_y, mute=True)\n",
    "\n",
    "        et_model = SklearnWrapper(clf=ExtraTreesRegressor, params={\n",
    "            'n_estimators': 100, 'random_state': 2019})\n",
    "        et_train_preds = et_model.train(train_x, train_y, splits, mute=True).reshape(-1, 1)\n",
    "        et_test_preds = et_model.predict_test(test_x, test_y, mute=True)\n",
    "        \n",
    "        gb_model = SklearnWrapper(clf=GradientBoostingRegressor, params={\n",
    "            'random_state': 2019})\n",
    "        gb_train_preds = gb_model.train(train_x, train_y, splits, mute=True).reshape(-1, 1)\n",
    "        gb_test_preds = gb_model.predict_test(test_x, test_y, mute=True)\n",
    "\n",
    "        train_second_layer = np.concatenate((\n",
    "            train_x, rf_train_preds, et_train_preds, gb_train_preds), axis=1)\n",
    "        test_second_layer = np.concatenate((\n",
    "            test_x, rf_test_preds[:, None], et_test_preds[:, None], \n",
    "            gb_test_preds[:, None]), axis=1)\n",
    "\n",
    "        et2_model = SklearnWrapper(clf=ExtraTreesRegressor, params=second_layer_params)\n",
    "        et2_train_preds = et2_model.train(train_second_layer, train_y, splits, mute=True).reshape(-1, 1)\n",
    "        et2_test_preds = et2_model.predict_test(test_second_layer, test_y, mute=True)\n",
    "        \n",
    "        return et2_train_preds, et2_test_preds, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above pipeline, all that we need to do now is modify the distribution throughout the 2015 test set and see whether or not we can reduce the overall unmet need. In order to do so, we'll specifically take the sum of the given distribution and redistribute it as necessary, specifically ensuring that we don't end up distributing more than the amount that we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data = pd.read_csv(\"Capital_Area_Food_Bank_Hunger_Estimates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "f15_columns = ['F15_FI_RATE', 'F15_LB_UNME', 'F15_DISTRIB', 'F15_PPIN']\n",
    "f14_columns = ['F14_FI_RATE', 'F14_LB_UNME', 'F14_DISTRIB', 'F14_PPIN']\n",
    "\n",
    "f14_initial = [col for col in food_data.columns if col not in f14_columns]\n",
    "f14_final = [col for col in f14_initial if col not in f15_columns] + f14_columns\n",
    "f15_initial = [col for col in food_data.columns if col not in f15_columns]\n",
    "f15_final = [col for col in f15_initial if col not in f14_columns] + f15_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data_2014 = food_data[f14_final]\n",
    "food_data_2015 = food_data[f15_final]\n",
    "\n",
    "f14_rename = {'F14_FI_RATE': \"FI_RATE\", 'F14_LB_UNME': \"LB_UNME\", 'F14_DISTRIB': \"DISTRIB\", 'F14_PPIN': \"PPIN\"}\n",
    "f15_rename = {'F15_FI_RATE': \"FI_RATE\", 'F15_LB_UNME': \"LB_UNME\", 'F15_DISTRIB': \"DISTRIB\", 'F15_PPIN': \"PPIN\"}\n",
    "cols_to_use = [\"LB_UNME\", \"FI_RATE\", \"TOTAL_POP\", \"PPIN\", \"DISTRIB\"]\n",
    "\n",
    "food_data_2014 = food_data_2014.rename(columns=f14_rename)[cols_to_use]\n",
    "food_data_2015 = food_data_2015.rename(columns=f15_rename)[cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Unmet Need (Difference): 3525342.068632804\n"
     ]
    }
   ],
   "source": [
    "second_params = {}\n",
    "second_params[\"n_estimators\"] = 200\n",
    "second_params[\"max_depth\"] = 5\n",
    "second_params[\"random_state\"] = 2019\n",
    "\n",
    "pipeline = EnsemblePipeline()\n",
    "train, test = pipeline.transform(food_data_2014, food_data_2015)\n",
    "train_preds, test_preds, test_y = pipeline.gen_predictions(train, test, second_params)\n",
    "print(\"Calculated Unmet Need (Difference): {}\".format(sum(test_preds) - sum(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial difference of unmet need from the predictions comes out to be approximately 3525342.07. We'll go and set this value as the \"zero\" and see if we can go and improve on this result with the addition of noise and a few other strategies intended to minimize this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34029764.92214297\n"
     ]
    }
   ],
   "source": [
    "print(sum(food_data_2015[\"DISTRIB\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have just over 34,000,000 pounds of food to distribute. One way that we could do such is to simply look at the population of each individual district and the percentage of individuals in the district whom are experiencing food insecurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_population = food_data_2015[\"TOTAL_POP\"].values\n",
    "insecurity_rate = food_data_2015[\"FI_RATE\"].values\n",
    "\n",
    "district_percentages = [x * y for x, y in zip(district_population, insecurity_rate)]\n",
    "normalized_percentages = [(x - min(district_percentages)) / (max(district_percentages) - min(district_percentages)) for x in district_percentages]\n",
    "distribution_values = [sum(food_data_2015[\"DISTRIB\"]) * x / (sum(normalized_percentages)) for x in normalized_percentages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34029764.92214292\n"
     ]
    }
   ],
   "source": [
    "food_data_2015[\"DISTRIB\"] = distribution_values\n",
    "print(sum(food_data_2015[\"DISTRIB\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Unmet Need (Difference): 2632726.583172731\n"
     ]
    }
   ],
   "source": [
    "second_params = {}\n",
    "second_params[\"n_estimators\"] = 200\n",
    "second_params[\"max_depth\"] = 1\n",
    "second_params[\"random_state\"] = 2019\n",
    "            \n",
    "pipeline = EnsemblePipeline()\n",
    "train, test = pipeline.transform(food_data_2014, food_data_2015)\n",
    "train_preds, test_preds, test_y = pipeline.gen_predictions(train, test, second_params)\n",
    "print(\"Calculated Unmet Need (Difference): {}\".format(sum(test_preds) - sum(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reworking the overall distribution methodology, we were able to get a unmet need calculation of 2632726.58, down from 3525342.07 in the original simulation. Although we're not exactly able to get a complete idea of how well the new methodology will work throughout, given that the only true labels we're able to get are the ones for the original, the calculated unmet need with this distribution is expected to be approximately 892615.49 less than it would have been, going to show that Machine Learning truly can help with problems such as these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
